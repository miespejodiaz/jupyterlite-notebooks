{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "935cfb8a-e241-4d88-90c0-105c2cbc42be",
      "cell_type": "code",
      "source": "#!/usr/bin/env nextflow\n\nimport groovy.json.JsonBuilder\nnextflow.enable.dsl = 2\n\ninclude { fastq_ingress; xam_ingress } from './lib/ingress'\ninclude {\n    getParams;\n} from './lib/common'\n\n\nOPTIONAL_FILE = file(\"$projectDir/data/OPTIONAL_FILE\")\n\n\nprocess getVersions {\n    label \"wf_teloseq\"\n    cpus 1\n    memory 2.GB\n    output: path \"versions.txt\"\n    script:\n    \"\"\"\n    python -c \"import Bio; print(f'biopython,{Bio.__version__}')\" >> versions.txt\n    python -c \"import matplotlib as mpl; print(f'matplotlib,{mpl.__version__}')\" >> versions.txt\n    python -c \"import pyfastx; print(f'pyfastx,{pyfastx.__version__}')\" >> versions.txt\n    python -c \"import pysam; print(f'pysam,{pysam.__version__}')\" >> versions.txt\n    python -c \"import pandas as pd; print(f'pandas,{pd.__version__}')\" >> versions.txt\n    python -c \"import numpy as np; print(f'numpy,{np.__version__}')\" >> versions.txt\n    seqkit version | grep \"eqkit\" | awk '{print \"seqkit,\" \\$2}' >> versions.txt\n    #pip show edlib | grep Version | awk '{print \"edlib,\" \\$2}' >> versions.txt\n    samtools --version | grep samtools | head -1 | sed 's/ /,/' >> versions.txt\n    csvtk version | awk '{print \"csvtk,\" \\$2}' >> versions.txt\n    minimap2 --version | awk '{print \"minimap2,\" \\$1}' >> versions.txt\n    cutadapt --version | awk '{print \"cutadapt,\" \\$1}' >> versions.txt\n    vsearch --version 2>&1 | grep \"vsearch \" | sed 's/,.*//' | sed 's/ /,/' | sed 's/_.*//' >> versions.txt\n    #seqtk 2>&1 | grep \"Version\" | awk '{print \"seqtk,\" \\$2}' >> versions.txt\n    \n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2d6388d3-c70e-4342-b140-4fd5345a2814",
      "cell_type": "code",
      "source": "//This process combines the first --denovo output reference with the user supplied contigs and error corrects and renames the entire combined reference.\nprocess manualCuration {\n    label \"wf_teloseq\"\n    cpus = 6\n    memory 2.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\"), path(\"coverage.txt\")\n        path(\"new_contigs.fasta\")\n        path(\"denovo_reference_round1.fasta\")\n        tuple val(meta2), path(\"reference_used_for_naming.fasta\")\n        val(enzyme)\n    output:\n        tuple val(meta), path(\"Manual_denovo_reference.fasta\"), emit: ref1\n    script:\n    \"\"\"\n\n    extend_telomere.py new_contigs.fasta new_contigs2.fasta\n    cutadapt --cut -6 denovo_reference_round1.fasta > reference2.fasta\n    cat new_contigs2.fasta reference2.fasta > reference_manual.fasta\n    minimap2 -ax map-ont -t $task.cpus reference_manual.fasta reads.fastq | samtools sort -o Telomere.bam\n    samtools index Telomere.bam\n    samtools faidx reference_manual.fasta\n    freebayes -f reference_manual.fasta Telomere.bam >varnew.vcf\n    bcftools view -i 'GT=\"1/1\"' varnew.vcf -o filtered.vcf\n    bgzip -c filtered.vcf > filtered.vcf.gz\n    tabix -p vcf filtered.vcf.gz\n    bcftools consensus -f reference_manual.fasta filtered.vcf.gz -o consensus.fasta\n\n    minimap2 -ax map-ont -t $task.cpus -y --MD --sam-hit-only --eqx consensus.fasta reads.fastq | samtools sort -o Telomere2.bam\n    samtools index Telomere2.bam\n\n    redundent_contig_removal_cov.py Telomere2.bam coverage.txt > idlisttoremove.txt\n    #mosdepth -n -t 8 --fast-mode lowcov2 Telomere2.bam\n    #awk -v cov=\"\\$cov\" '{if (\\$4 > cov) print \\$1}' lowcov2.mosdepth.summary.txt > idlisttokeep.txt\n    #awk '{if (\\$4 > cov) print \\$1}' lowcov2.mosdepth.summary.txt > idlisttokeep.txt\n    seqkit grep -v -f idlisttoremove.txt consensus.fasta > consensus_final.fasta\n\n    #cutadapt -g \"TAACCCTAACCCTAACCCTAACCCTAACCC;rightmost\" -e 0 -o correctedref.fasta reference_used_for_naming.fasta\n    #blast_rename_ref.py -db correctedref.fasta -q consensus_final.fasta -o Manual_denovo_reference.fasta -i ${baseDir}/test_data/HG002_groupings.csv\n    \n    awk '/^>/{if(seq!=\"\"){print seq \"$enzyme\"}; print; seq=\"\"; next} {seq=seq\"\"\\$0} END{print seq \"$enzyme\"}' consensus_final.fasta > consensus_final2.fasta\n    awk '/^>/ {print \">contig_\"++i; next} {print}' consensus_final2.fasta > Manual_denovo_reference.fasta\n\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c8a19d64-9397-4858-a81e-3e5cd77a1d2a",
      "cell_type": "code",
      "source": "process coverage_calc {\n    label \"wf_teloseq\"\n    cpus 1\n    memory 2.GB\n    input:\n        tuple val(meta), path(reads)\n    output:\n        tuple val(meta), path(\"coverage.txt\")\n    script:\n    \"\"\"\n    if [[ \"${params.mincoverage}\" != -1 ]]; then\n        echo \"${params.mincoverage}\" > coverage.txt\n    else\n        read_count=\\$(wc -l < $reads | awk '{print \\$1}')\n        echo \"\\$((read_count / 4 / 92 * 20 / 100 ))\" > coverage.txt\n    fi\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4f128b1a-e401-4897-a917-89f6f12ab2e6",
      "cell_type": "code",
      "source": "process rmdup {\n    label \"wf_teloseq\"\n    cpus = 1\n    memory 5.GB\n    input:\n        tuple val(meta), path(\"reads.fastq.gz\")\n    output:\n        tuple val(meta), path(\"dedup.fastq\")\n    script:\n    \"\"\"\n    seqkit rmdup -n reads.fastq.gz > dedup.fastq\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "282e5dff-00db-468c-b7fa-88104e7b222d",
      "cell_type": "code",
      "source": "process rmdup2 {\n    label \"wf_teloseq\"\n    cpus = 1\n    memory 5.GB\n    input:\n        tuple val(meta), path(\"reads.fastq.gz\")\n    output:\n        tuple val(meta), path(\"dedup.fastq\")\n    script:\n    \"\"\"\n    seqkit rmdup -n reads.fastq.gz > dedup.fastq\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "49a73bb1-61f9-491d-85f7-ae7825384954",
      "cell_type": "code",
      "source": "process subtelomere {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 5.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"subtelomere.txt\")\n    script:\n    \"\"\"\n    cutadapt -g \"TAACCCTAACCCTAACCCTAACCCTAACCC;rightmost\" -e 0 -o subtelomere.fastq reads.fastq\n    awk 'NR%4 == 2 {print length(\\$0)}' subtelomere.fastq > subtelomere2.txt\n    tr -s ' ' '\\t' < subtelomere2.txt > subtelomere.txt\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8b672169-e954-4952-bbc2-f94287cb3a86",
      "cell_type": "code",
      "source": "//remove short reads, could add quality filter too -Q, ALT pathway results in very short telomere so may not need this?\nprocess remove_short1 {\n    // TODO: ingress allows filtering of reads based on length + Q scores with fastcat\n    // extra args\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 8.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"reads.short.fastq\")\n    script:\n    \"\"\"\n    seqkit seq -m 100 -Q ${params.read_quality} reads.fastq -o reads.short.fastq\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "11920b7b-7c6c-4c12-8f52-d08f7db9fabf",
      "cell_type": "code",
      "source": "//remove short reads, could add quality filter too -Q\nprocess remove_short2 {\n    // TODO: ingress allows filtering of reads based on length + Q scores with fastcat\n    // extra args\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 8.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"reads.short.fastq\")\n    script:\n    \"\"\"\n    seqkit seq -m 100 -Q ${params.read_quality} reads.fastq -o reads.short.fastq\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "33e39270-5dea-4d2d-875b-af6e6456a225",
      "cell_type": "code",
      "source": "process check_reference {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 32.GB\n    input:\n        path(ref)\n        val(enzyme)\n    output:\n        tuple val('reference_user'), path(\"reference.fasta\"), emit: ref1\n    publishDir \"${params.out_dir}/reference/\", mode: 'copy', overwrite: true\n    // TODO: do we have to publish here instead of using the canonical way (with the\n    // `output` process)?\n    script:\n    \"\"\"\n    ##Extending telomere to all references as observed misclassification of primary and secondary if some arms have telomeres\n    ##long enough for mapping but the other similar site doesn't then that softclipping is taken into consideration\n    ##and the primary can then become secondary and vice versa\n    # TODO: the python scripts should probably use `pysam` instead of `biopython` as it\n    # implicitly deals with compressed and uncompressed FASTx files\n    if [[ $ref == *.gz ]]; then\n        zcat $ref > reference.fasta \n        extract_reference.py reference.fasta $enzyme\n        extend_telomere.py reference2.fasta reference.fasta\n    elif [[ $ref == *.fasta ]]; then\n        extract_reference.py $ref $enzyme\n        extend_telomere.py reference2.fasta reference.fasta\n    elif [[ $ref == *.fa ]]; then\n        extract_reference.py $ref $enzyme\n        extend_telomere.py reference2.fasta reference.fasta\n    else\n        pass\n    fi\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e21ec62f-42f7-42e4-9ff4-31e4b23e0bbe",
      "cell_type": "code",
      "source": "process mapAndSeparateR1 {\n    label \"wf_teloseq\"\n    cpus = 6\n    memory 2.GB\n    input:\n        tuple val(meta), path(input)\n        tuple val(meta2), path(\"subtelomere_reference.fasta\")\n        val(cov)\n    output:\n        tuple val(meta), path('*.txt'), emit: clusters\n\n    // to keep contigs to reference changed from this GGGGCGCGCAGCGCCGGCG\n    script:\n    \"\"\"\n    #remove telomere from reference used\n    cutadapt -g \"TAACCCTAACCCTAACCCTAACCCTAACCC;rightmost\" -e 0 -o Hg002refsubtelomere.fa subtelomere_reference.fasta\n    #remove telomere telomere cleaned reads\n    cutadapt -g \"TAACCCTAACCCTAACCCTAACCCTAACCC;rightmost\" -e 0 -o trimmedreadscleaned.fastq $input\n    #map cleaned subtelomere reads to subtelomere reference\n    minimap2 -ax map-ont -t $task.cpus -y --MD --sam-hit-only --eqx Hg002refsubtelomere.fa $input | samtools sort -o alignment.bam\n    samtools index alignment.bam\n    #get read information from mapped bam\n    seqkit bam alignment.bam 2>alignmentseqkit\n    #get fastq from mapped reads\n    samtools fastq -F 4 alignment.bam > alignment.fastq\n    #identify repeating motif\n    seqkit locate --only-positive-strand -m 0 -p CGCCGGCGCAGGCG alignment.fastq > alignment.motif\n    #summarise per read the repeating motif number\n    tail -n +2 alignment.motif | cut -f1 | sort | uniq -c | awk '{print \\$2 \"\\t\" \\$1}' > alignment.motifcounts\n    #separate out mismapped reads, commented out left clip separation in this script as only useful in high coverage situations\n    clustera.py alignmentseqkit alignment.motifcounts $cov\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "535b7ae4-88ef-444e-8560-41a3494d97ce",
      "cell_type": "code",
      "source": "process clusterAndExtractR1 {\n    label \"wf_teloseq\"\n    cpus = 4\n    memory 3.GB\n    input:\n        tuple val(meta), path(clusterFiles), path(input)\n        val(cov)\n    output:\n        tuple val(meta), path (\"${clusterFiles}.set2.consensus_highestseqs.fasta\") , emit: results\n\n    script:\n    \"\"\"\n    seqkit grep -f $clusterFiles $input > ${clusterFiles}.fastq\n    seqtk seq -A ${clusterFiles}.fastq > ${clusterFiles}.set2.fasta\n    vsearch --cluster_fast ${clusterFiles}.set2.fasta --strand plus --threads $task.cpus --maxseqlength 200000 --id 0.96 --consout ${clusterFiles}.set2.consensus.fasta\n    extract_highest_seqs.py ${clusterFiles}.set2.consensus.fasta $cov\n\n    # Check if output file is empty\n    if [ ! -s ${clusterFiles}.set2.consensus_highestseqs.fasta ]; then\n        # If empty, create an empty file\n        touch ${clusterFiles}.set2.consensus_highestseqs.fasta\n    fi\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6016105c-13d2-4e3f-b75c-304231c9f3d5",
      "cell_type": "code",
      "source": "process clusterAndExtractR2 {\n    // TODO: instead of duplicating these processes, they could be defined in a separate\n    // `.nf` file and imported several times\n    label \"wf_teloseq\"\n    cpus = 4\n    memory 0.3.GB\n    input:\n        tuple val(meta), path(clusterFiles), path(input)\n        val(cov)\n    output:\n        tuple val(meta), path (\"${clusterFiles}.set2.consensus_highestseqs.fasta\") , emit: results\n    script:\n    \"\"\"\n    seqkit grep -f $clusterFiles $input > ${clusterFiles}.fastq\n    seqtk seq -A ${clusterFiles}.fastq > ${clusterFiles}.set2.fasta\n    # --cluster_fast removed to reduce possibility of redundent contigs\n    vsearch --cluster_fast ${clusterFiles}.set2.fasta --strand plus --threads $task.cpus --maxseqlength 200000 --id 0.96 --consout ${clusterFiles}.set2.consensus.fasta\n    extract_highest_seqs.py ${clusterFiles}.set2.consensus.fasta $cov\n\n    # Check if output file is empty\n    if [ ! -s ${clusterFiles}.set2.consensus_highestseqs.fasta ]; then\n        # If empty, create an empty file\n        touch ${clusterFiles}.set2.consensus_highestseqs.fasta\n    fi\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8665ffb6-ea51-462c-b78a-c006eda4d0ab",
      "cell_type": "code",
      "source": "process clusterAndExtractR3 {\n    label \"wf_teloseq\"\n    cpus = 4\n    memory 0.3.GB\n    input:\n        tuple val(meta), path(clusterFiles), path(input)\n        val(cov)\n        \n    output:\n        tuple val(meta), path (\"${clusterFiles}.set2.consensus_highestseqs.fasta\") , emit: results\n    script:\n    \"\"\"\n    seqkit grep -f $clusterFiles $input > ${clusterFiles}.fastq\n    seqtk seq -A ${clusterFiles}.fastq > ${clusterFiles}.set2.fasta\n    # --cluster_fast removed to reduce possibility of redundent contigs\n    vsearch --cluster_fast ${clusterFiles}.set2.fasta --strand plus --threads $task.cpus --maxseqlength 200000 --id 0.96 --consout ${clusterFiles}.set2.consensus.fasta\n    extract_highest_seqs.py ${clusterFiles}.set2.consensus.fasta $cov\n\n    # Check if output file is empty\n    if [ ! -s ${clusterFiles}.set2.consensus_highestseqs.fasta ]; then\n        # If empty, create an empty file\n        touch ${clusterFiles}.set2.consensus_highestseqs.fasta\n    fi\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "28563c8a-6986-402f-82f6-d0c34567259b",
      "cell_type": "code",
      "source": "process combineRefR1 {\n    label \"wf_teloseq\"\n    cpus = 6\n    memory 8.GB\n    input: \n        tuple val(meta), path(input), path(resultFiles)\n        val(cov)\n    output:\n        tuple val(meta), path(\"denovo_reference.fasta\"), emit: ref1\n        tuple val(meta), path(\"cutsites.bed\"), emit: cutbed\n        tuple val(meta), path(\"Telomere2.bam\"), path(\"Telomere2.bam.bai\"), emit: bam\n    \"\"\"\n    cat $resultFiles | deduplicate.py new.fasta\n    #trim all reads to TAACCC then add 5kbp of telomere\n    extend_telomere.py new.fasta new2.fasta\n    #map back original reads to polish 1|1 snp/indels\n    minimap2 -ax map-ont -t $task.cpus new2.fasta $input | samtools sort -o Telomere.bam\n    samtools index Telomere.bam\n    samtools faidx new2.fasta\n    freebayes -f new2.fasta Telomere.bam >varnew.vcf\n    #script to filter but subset based upon coverage\n    correct.py varnew.vcf new2.fasta predenovo_reference_temp.fasta\n    cat predenovo_reference_temp.fasta | deduplicate.py predenovo_reference.fasta\n    minimap2 -ax map-ont -t $task.cpus predenovo_reference.fasta $input | samtools sort -o Telomere2.bam\n    samtools index Telomere2.bam\n\n    redundent_contig_removal.py Telomere2.bam $cov > idlisttoremove.txt\n    seqkit grep -v -f idlisttoremove.txt predenovo_reference.fasta > denovo_reference.fasta\n    samtools faidx denovo_reference.fasta\n    awk '{print \\$1\"\\t\"\\$2\"\\t\"\\$2}' denovo_reference.fasta.fai > cutsites.bed\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "29f4a400-3497-49c0-93c4-9d3d4bc50756",
      "cell_type": "code",
      "source": "process combineRefR2 {\n    label \"wf_teloseq\"\n    cpus = 6\n    memory 8.GB\n    input:\n        tuple val(meta), path(input), path(resultFiles), path(ref), path(\"coverage.txt\")\n        tuple val(meta2), path(\"ref_original.fasta\")\n        val(enzyme)\n    output:\n        tuple val(meta), path(\"denovo_reference.fasta\"), emit: ref1\n        tuple val(meta), path(\"cutsites.bed\"), emit: cutbed\n        tuple val(meta), path(\"Raw_read.bam\"), path(\"Raw_read.bam.bai\"), emit: bam\n    \"\"\"\n    cat $resultFiles > tmpref.fa\n    extend_telomere.py tmpref.fa tmpref2.fa\n    cat $ref tmpref2.fa > new2.fasta\n    #map back original reads to polish 1|1 snp/indels\n    minimap2 -ax map-ont -t $task.cpus new2.fasta $input | samtools sort -o Telomere.bam\n    samtools index Telomere.bam\n    samtools faidx new2.fasta\n    #SHOULD I BE FILTERING BAM BEFORE THIS STAGE TO REMOVE LOW CONFIDENCE MAPPING?\n    freebayes -f new2.fasta Telomere.bam >varnew.vcf\n    #script to correct snp/indels in reference\n    correct.py varnew.vcf new2.fasta predenovo_reference_temp.fasta\n    #remove duplicate contigs using sequence that have come about by error in clustering contigs that already exist. Ignore first 80bp when comparing sequences\n    cat predenovo_reference_temp.fasta | deduplicate.py predenovo_reference.fasta\n    #map again\n    minimap2 -ax map-ont -t $task.cpus predenovo_reference.fasta $input | samtools sort -o Raw_read.bam\n    samtools index Raw_read.bam\n    #remove duplicate contigs using coverage that have come about by error in clustering contigs that already exist\n    redundent_contig_removal_cov.py Raw_read.bam coverage.txt > idlisttoremove.txt\n    seqkit grep -v -f idlisttoremove.txt predenovo_reference.fasta > denovo_reference2.fasta\n\n    #Add lost cut site for future pipeline use to end of contigs\n    awk '/^>/{if(seq!=\"\"){print seq \"$enzyme\"}; print; seq=\"\"; next} {seq=seq\"\"\\$0} END{print seq \"$enzyme\"}' denovo_reference2.fasta > denovo_reference3.fasta\n    #rename contigs ADD WITH CHR NAMING IN FUTURE\n    awk '/^>/ {print \">contig_\"++i; next} {print}' denovo_reference3.fasta > denovo_reference.fasta\n    #create cutsites file showing where reads should map up to for later use\n    samtools faidx denovo_reference.fasta\n    awk '{print \\$1\"\\t\"\\$2\"\\t\"\\$2}' denovo_reference.fasta.fai > cutsites.bed\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ec55b032-820a-49cd-88b1-e000b6d68fac",
      "cell_type": "code",
      "source": "//combineRefR3\nprocess combineRefR3 {\n    label \"wf_teloseq\"\n    cpus = 6\n    memory 8.GB\n    input: \n        tuple val(meta), path(input), path(resultFiles), path(ref), path(\"coverage.txt\")\n        tuple val(meta2), path(\"ref_original.fasta\")\n        val(enzyme)\n    output:\n        tuple val(meta), path(\"denovo_reference.fasta\"), emit: ref1\n        tuple val(meta), path(\"cutsites.bed\"), emit: cutbed\n        tuple val(meta), path(\"Raw_read.bam\"), path(\"Raw_read.bam.bai\"), emit: bam\n    \"\"\"\n    cat $resultFiles > tmpref.fa\n    extend_telomere.py tmpref.fa tmpref2.fa\n    cat $ref tmpref2.fa > new2.fasta\n    #map back original reads to polish 1|1 snp/indels\n    minimap2 -ax map-ont -t $task.cpus new2.fasta $input | samtools sort -o Telomere.bam\n    samtools index Telomere.bam\n    samtools faidx new2.fasta\n\n    freebayes -f new2.fasta Telomere.bam >varnew.vcf\n    #script to correct snp/indels in reference\n    correct.py varnew.vcf new2.fasta predenovo_reference_temp.fasta\n    #remove duplicate contigs using sequence that have come about by error in clustering contigs that already exist. Ignore first 80bp when comparing sequences\n    cat predenovo_reference_temp.fasta | deduplicate.py predenovo_reference.fasta\n    #map again\n    minimap2 -ax map-ont -t $task.cpus predenovo_reference.fasta $input | samtools sort -o Raw_read.bam\n    samtools index Raw_read.bam\n    #remove duplicate contigs using coverage that have come about by error in clustering contigs that already exist\n    redundent_contig_removal_cov.py Raw_read.bam coverage.txt > idlisttoremove.txt\n    seqkit grep -v -f idlisttoremove.txt predenovo_reference.fasta > denovo_reference2.fasta\n\n    #rename final reference contigs, first remove variable regions #note could improve this further as can pair up within this naming and group further on similarity.\n    #DONT USE AT MOMENT NEEDS FURTHER WORK\n    #cutadapt -g \"TAACCCTAACCCTAACCCTAACCCTAACCC;rightmost\" -e 0 -o correctedref.fasta ref_original.fasta\n    #blast_rename_ref.py -db correctedref.fasta -q denovo_reference2.fasta -o denovo_reference5.fasta -i ${baseDir}/test_data/HG002_groupings.csv\n    \n    #Add lost cut site for future pipeline use to end of contigs\n    awk '/^>/{if(seq!=\"\"){print seq \"$enzyme\"}; print; seq=\"\"; next} {seq=seq\"\"\\$0} END{print seq \"$enzyme\"}' denovo_reference2.fasta > denovo_reference3.fasta\n    #rename contigs ADD WITH CHR NAMING IN FUTURE\n    awk '/^>/ {print \">contig_\"++i; next} {print}' denovo_reference3.fasta > denovo_reference.fasta\n    #create cutsites file showing where reads should map up to for later use\n    samtools faidx denovo_reference.fasta\n    awk '{print \\$1\"\\t\"\\$2\"\\t\"\\$2}' denovo_reference.fasta.fai > cutsites.bed\n    \"\"\"\n}\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b0826909-4451-46d0-8eb3-bde91e4a9f84",
      "cell_type": "code",
      "source": "process mapAndSeparateR2 {\n    label \"wf_teloseq\"\n    cpus = 6\n    memory 2.GB\n    input:\n        tuple val(meta), path(\"alignment.bam\"), path(\"alignment.bam.bai\")\n        val(cov)\n    output:\n        tuple val(meta), path('*_1.txt'), emit: clusters\n    script:\n    \"\"\"\n    #get read information from mapped bam\n    seqkit bam alignment.bam 2>alignmentseqkit\n    #get fastq from mapped reads\n    samtools fastq alignment.bam > alignment.fastq\n    #identify repeating motif but not using this now so could be removed with cluster script change...\n    seqkit locate --only-positive-strand -m 0 -p CGCCGGCGCAGGCG alignment.fastq > alignment.motif\n    #summarise per read the repeating motif number\n    indel_count.py\n    tail -n +2 alignment.motif | cut -f1 | sort | uniq -c | awk '{print \\$2 \"\\t\" \\$1}' > alignment.motifcounts\n    #separate out mismapped reads using just motif and pos\n    cluster2b.py alignmentseqkit alignment.motifcounts $cov indel_counts.txt\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d10753cb-2c41-4d55-ba01-a3ead0b3885c",
      "cell_type": "code",
      "source": "process mapAndSeparateR3 {\n    label \"wf_teloseq\"\n    cpus 2\n    memory 2.GB\n    input:\n        tuple val(meta), path(\"alignment.bam\"), path(\"alignment.bam.bai\")\n        val(cov)\n    output:\n        tuple val(meta), path('*_1.txt'), emit: clusters\n    //set minimum 30 as clustered reads to produce reference and 20 coverage at the end to keep contigs to reference\n    script:\n    \"\"\"\n    #get read information from mapped bam\n    seqkit bam alignment.bam 2>alignmentseqkit\n    #get fastq from mapped reads\n    samtools fastq alignment.bam > alignment.fastq\n    #identify repeating motif but not using this now so could be removed with cluster script change...\n    seqkit locate --only-positive-strand -m 0 -p CGCCGGCGCAGGCG alignment.fastq > alignment.motif\n    #summarise per read the repeating motif number\n    indel_count.py\n    tail -n +2 alignment.motif | cut -f1 | sort | uniq -c | awk '{print \\$2 \"\\t\" \\$1}' > alignment.motifcounts\n    #separate out mismapped reads using just motif and pos\n    cluster2b.py alignmentseqkit alignment.motifcounts $cov indel_counts.txt\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9408ca32-c84b-4571-a317-67a146fd693e",
      "cell_type": "code",
      "source": "##Aquí empieza el filtering ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5bbc459b-21bf-4247-98c8-47996a1e08fc",
      "cell_type": "code",
      "source": "process checkFastq {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 0.5.GB\n    input:\n        tuple val(meta), path(input)\n    output:\n        tuple val(meta), path(\"teloseq.fastq.gz\"), emit: fastqfile\n    //adds time when not gzipped files, could be done better\n    script:\n    \"\"\"\n    if [[ ${input.getExtension()} == \"fastq\" ]]; then\n        echo \"Input is already in FASTQ format\"\n        cp $input teloseq.fastq\n        gzip teloseq.fastq\n    elif [[ ${input.getExtension()} == \"gz\" ]]; then\n        cp $input teloseq.fastq.gz\n    elif [[ ${input.getExtension()} == \"bam\" || ${input.getExtension()} == \"sam\" ]]; then\n        samtools fastq $input > teloseq.fastq\n        gzip teloseq.fastq\n    else \n        echo \"Unsupported input format: ${input.getExtension()}\"\n    fi\n    \"\"\"\n}\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9554e4cc-0e68-4ba3-ab2c-be5de942f06a",
      "cell_type": "code",
      "source": "process trim_adapters {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 2.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"reads_trimmed.fastq\"), emit: fastqtrimmed\n    //trim adapter. why not use cutadapt, because adapter not basecalled well and precision important to end motifs\n    \"\"\"\n    trim_adapter2.py reads.fastq reads_trimmed.fastq\n    \"\"\"\n\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3d8a0e96-f2ab-46f2-9b31-d6c05867709a",
      "cell_type": "code",
      "source": "##El comando busca todas las lecturas en reads.fastq que contengan esta repetición específica \n##dentro de las primeras 60-500 bases y las guarda en un nuevo archivo llamado telomere.fastq.\n\nprocess filter_telomeres {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 8.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"telomere.fastq\")\n    //identify telomere x10 repeat containing reads within the first 60-500bp as sequencing from 5' the telomere so should be there if telomere read\n    \"\"\"\n    seqkit grep -s -R 60:500 -P -p \"TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCC\" reads.fastq > telomere.fastq\n    \"\"\"\n}\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ebc31058-bb2c-437a-980a-410e3b76c0d2",
      "cell_type": "code",
      "source": "##the same command, I don't know why\nprocess filter_telomeres2 {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 8.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"telomere.fastq\")\n    //identify telomere x10 repeat containing reads within the first 60-500bp as sequencing from 5' the telomere so should be there if telomere read\n    \"\"\"\n    seqkit grep -s -R 60:500 -P -p \"TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCC\" reads.fastq > telomere.fastq\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8d423431-f833-4f36-87aa-4c6ed76f217e",
      "cell_type": "code",
      "source": "##This command processes the input FASTQ file by reverse complementing all the sequences in it. The resulting sequences are saved \n##into a new file (telomere.fastq). The metadata (meta) is passed through unchanged.\nprocess reversecomplement {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 0.5.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"telomere.fastq\")\n    //identify telomere x10 repeat containing reads within the first 60-500bp as sequencing from 5' the telomere so should be there if telomere read\n    \"\"\"\n    seqtk seq -r reads.fastq > telomere.fastq\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "32e8fc6e-db73-42ef-b629-2b0abe6f5c83",
      "cell_type": "code",
      "source": "##This process concatenates two FASTQ files (reads.fastq and reads2.fastq) into one combined FASTQ file (telomere.fastq).\nprocess combinefastq {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 0.5.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n        tuple val(meta), path(\"reads2.fastq\")\n    output:\n        tuple val(meta), path(\"telomere.fastq\")\n    //identify telomere x10 repeat containing reads within the first 60-500bp as sequencing from 5' the telomere so should be there if telomere read\n    \"\"\"\n    cat reads.fastq reads2.fastq > telomere.fastq\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "87c8b0ab-3112-4a52-8f0a-b379eb7e986e",
      "cell_type": "code",
      "source": "##The command filters out any reads from the reads.fastq file that contain the telomere sequence (\"TAACCCTAACCCTAACCCTAACCC\") in the last 70 bases, writing the remaining sequences\n##(those without this telomere sequence at the end) to a new file telomere_and_non_telomere.fastq.The process is designed to avoid sequences that are too short or represent artifacts.\n\nprocess filter_nontelomeres {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 8.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"telomere_and_non_telomere.fastq\")\n    //filter further subset that should not have telomere sequence in the last 70bp as shortest cut site is beyond this so should have atleast this amount of sequence at end that\n    //is not telomere and don't want telomere only reads as could be fragments or artefacts. \n    \"\"\"\n    seqkit grep -v -s -R -70:-1 -P -p \"TAACCCTAACCCTAACCCTAACCC\" reads.fastq > telomere_and_non_telomere.fastq\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9eb239d5-ac55-44fd-b184-564259dca446",
      "cell_type": "code",
      "source": "###ATTENTION: comando raro###\n##Este comando no me gusta porque te quita potenciales mutaciones, no se porqué\n\nprocess filter_motifs {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 8.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"removereadids.txt\")\n    //seqkit forward strand the telomere repeat and identify all locations for each read\n    //reverse the file so last telomere location for each read is first and print one row for each read, effectively getting last telomere match position for each read\n    //retrieve just id and length information I need\n    //search for basecalling error in telomere\n    //Identify reads with high intensity so lots of kmer error clustered error within the telomere as this will not map and get softclipped, plus not useful for telomere length.\n    //ensure just one read ID and @ is removed as not needed later on.\n    \"\"\"\n    seqkit locate --only-positive-strand -m 1 -p TAACCCTAACCCTAACCCTAACCCTAACCC reads.fastq > locationstelomere.txt\n    tac locationstelomere.txt | awk '!a[\\$1]++' > locationstelomerelast.txt\n    awk -F'\\t' '{print \\$1\" \"\\$5}' locationstelomerelast.txt | sort -r | tr ' ' '\\t' > locationstelomerelast2.txt\n\n    seqkit locate --only-positive-strand -m 0 -p GTATAG,CGCGCGCG,CCACCG,AGCGACAG,ATAAGT,CCTCGTCC,TATAGT,AGTACT,GAGTCC,TATAGT,TATACA,TGGTCC,CTCTCCTCT reads.fastq > motifs2.txt\n    error_reads.py motifs2.txt locationstelomerelast2.txt errors.txt\n\n    awk '!a[\\$1]++' errors.txt > removereadids.txt\n    sed -i 's/@//g' removereadids.txt\n\n]\n\n###Summary of what the error_reads.py script does:\n\nThe script takes two input files: one with motif information (positions of various motifs in the reads) and another with telomere positions for each read. \nIt identifies reads where motifs are clustered together (within a 500 base pair window). If a read has 5 or more motifs within this window, \nit suggests that the read might have basecalling errors or other artifacts, especially within the telomere region. The problematic read IDs are written \nto an output file (errors.txt), and these reads can be flagged for removal or further inspection.             \n\n###error_reads.py:\n#!/usr/bin/env python\n\"\"\"Script to process error reads data and print ID results to a file.\"\"\"\n\nimport sys\n\nimport numpy as np\nimport pandas as pd\n\ninput_file = sys.argv[1]\ninput_file2 = sys.argv[2]\n\n# Read the data from your file into a pandas DataFrame\ndf = pd.read_csv(\n    input_file,\n    delimiter='\\t',\n    header=None,\n    names=['seqID', 'patternName', 'pattern', 'strand', 'start', 'end', 'matched'],\n    skiprows=1\n)\n\n# Import reference fai as dataframe for list of chr that should be in list\nrawreadset = pd.read_csv(input_file2, sep=\"\\t\", header=None, names=['seqID', 'start'])\n\n# Merge rawreadset by seqID\ndf = df.merge(rawreadset, on='seqID')\n\n# Convert the 'start' columns to numeric\ndf['start_x'] = pd.to_numeric(df['start_x'], errors='coerce')\ndf['start_y'] = pd.to_numeric(df['start_y'], errors='coerce')\n\n# Remove rows where start_x is greater than start_y as errors only occur within telomere\ndf = df[df['start_x'] <= df['start_y']]\n\n# Group the DataFrame by 'seqID'\ngrouped = df.groupby('seqID')\n\n# Define the sliding window size (500 base pairs)\nwindow_size = 500\n\n# Prepare to write output to a file\noutput_file = 'errors.txt'\nwith open(output_file, 'w') as f:\n    # Iterate through groups and create a sliding window to count rows\n    for name, group in grouped:\n        start_positions = group['start_x'].to_numpy()\n        for start in start_positions:\n            count = (np.abs(start_positions - start) <= window_size).sum()\n            if count >= 5:\n                f.write(f'{name}\\n')\n}\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "545b7128-d6aa-4d74-9539-31647085e2c9",
      "cell_type": "code",
      "source": "//remove basecalling error reads from telomere only identified reads\nprocess filter_motifs_reads1 {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 8.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\"), path(\"remove_ids.txt\")\n    output:\n        tuple val(meta), path(\"Telomere_reads.fastq\")\n    \"\"\"\n    seqkit grep -v -f remove_ids.txt reads.fastq > Telomere_reads.fastq\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "08c09de8-fae3-467b-83d3-10f26521cec7",
      "cell_type": "code",
      "source": "//remove basecalling error reads from telomere and subtelomere identified reads\nprocess filter_motifs_reads2 {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 8.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\"), path(\"remove_ids.txt\")\n    output:\n        tuple val(meta), path(\"Telomere_reads.fastq\")\n    \"\"\"\n    seqkit grep -v -f remove_ids.txt reads.fastq > Telomere_reads.fastq\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3555421d-d382-4f79-99cd-9c95b08b75aa",
      "cell_type": "code",
      "source": "process fastq_stats {\n    // TODO: could we use the stats produced by `fastcat` during ingress instead?\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 5.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"raw.txt\")\n    //remove 2nd and third column not needed. and rename first column for report\n    \"\"\"\n    seqkit stats -a reads.fastq > stats3.txt\n    awk 'NR==2 { \\$1=\"Raw_Reads\" }1' stats3.txt > stats2.txt\n    awk 'BEGIN {OFS=\" \"} { \\$2=\\$3=\\$12=\"\"; print }' stats2.txt > raw2.txt\n    tr -s ' ' '\\t' < raw2.txt > raw.txt\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d210a821-f46d-491c-a597-931745a42369",
      "cell_type": "code",
      "source": "process fastq_stats2 {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 5.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"telomere.txt\")\n    //remove 2nd and third column not needed. and rename first column for report\n    \"\"\"\n    seqkit stats -a reads.fastq > stats3.txt\n    awk 'NR==2 { \\$1=\"Telomere_Reads\" }1' stats3.txt > stats2.txt\n    awk 'BEGIN {OFS=\" \"} { \\$2=\\$3=\\$12=\"\"; print }' stats2.txt > telomere2.txt\n    tr -s ' ' '\\t' < telomere2.txt > telomere.txt\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3b9ca543-a9ad-4659-bd5d-7ae0497e31da",
      "cell_type": "code",
      "source": "process fastq_stats3 {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 5.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n    output:\n        tuple val(meta), path(\"telomere_subtelomere.txt\")\n    //remove 2nd and third column not needed. and rename first column for report\n    \"\"\"\n    seqkit stats -a reads.fastq > stats3.txt\n    awk 'NR==2 { \\$1=\"Telomere_Subtelomere_Reads\" }1' stats3.txt > stats2.txt\n    awk 'BEGIN {OFS=\" \"} { \\$2=\\$3=\\$12=\"\"; print }' stats2.txt > telomere_subtelomere2.txt\n    tr -s ' ' '\\t' < telomere_subtelomere2.txt > telomere_subtelomere.txt\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e6ac2157-c749-4f8a-a3b6-5a2214a62556",
      "cell_type": "code",
      "source": "process mappingbam {\n    label \"wf_teloseq\"\n    cpus = 6\n    memory 64.GB\n    input:\n        tuple val(meta), path(\"reads.fastq\")\n        tuple val(meta2), path(\"mapping_reference2.fasta\")\n    output:\n        tuple val(meta), path(\"telomere.q${params.mapq}.bam\"),path(\"telomere.q${params.mapq}.bam.bai\"), emit: alignments\n        tuple val(meta), path(\"telomere.bam\"), path(\"telomere.bam.bai\"), emit: alignments2  // TODO: looks like this isn't used anywhere\n        tuple val(meta), path(\"mapping_reference.fasta\"), emit: mappingref\n    script:\n    \"\"\"\n    cp mapping_reference2.fasta mapping_reference.fasta  # TODO: why cp this?\n    minimap2 -ax map-ont --secondary=no --MD -t $task.cpus -y --sam-hit-only --eqx  mapping_reference.fasta reads.fastq > temp_aligned.sam\n    samtools sort -m 2G -O BAM -@ 4 temp_aligned.sam > telomere.bam\n    samtools index telomere.bam\n    samtools view -bq ${params.mapq} -h telomere.bam > \"telomere.q${params.mapq}.bam\"\n    samtools index \"telomere.q${params.mapq}.bam\"\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e966e8a6-9c22-47cd-8a92-6a0e0f6791c6",
      "cell_type": "code",
      "source": "//identify enzyme cut sites in reference, change -p sequence if different enzyme used\n//get first cut site in ref\n//tidy up file and remove header\nprocess cut_sites{\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 8.GB\n    input:\n        tuple val(meta2), path(\"cut_reference.fasta\")\n        val(enzyme)\n    output:\n        tuple val(meta2), path(\"cutfirst.bed\"), emit: cutbed\n    script:\n    \"\"\"       \n    seqkit locate --only-positive-strand -m 0 -p $enzyme cut_reference.fasta > cutsites2.txt\n    awk '!a[\\$1]++' cutsites2.txt > cutsites3.txt\n    awk -F'\\t' '{print \\$1\"\\t\"\\$5\"\\t\"\\$5}' cutsites3.txt > cutfirst4.txt\n    grep -v 'seqID' cutfirst4.txt > cutfirst.bed\n    \"\"\"\n}\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eee24541-8beb-4f11-81e6-a22e785a6edd",
      "cell_type": "code",
      "source": "//identify telomere end in reference, change -p sequence if different enzyme used\n//get last telomere site in ref\n//tidy up file and remove header\nprocess telomere_sites{\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 8.GB\n    input:\n        tuple val(meta2), path(\"tel_reference.fasta\")\n    output:\n        tuple val(meta2), path(\"locationstelomerelast2.bed\"), emit: telomerebed\n    script:\n    \"\"\"       \n    seqkit locate --only-positive-strand -m 1 -p TAACCCTAACCCTAACCCTAACCCTAACCC,AACCCTAACCCTAACCCTAACCCTAACCCT,ACCCTAACCCTAACCCTAACCCTAACCCTA,CCCTAACCCTAACCCTAACCCTAACCCTAA,CCTAACCCTAACCCTAACCCTAACCCTAAC,CTAACCCTAACCCTAACCCTAACCCTAACC tel_reference.fasta > locationstelomere.txt\n    tac locationstelomere.txt | awk '!a[\\$1]++' > locationstelomerelast.txt\n    awk -F'\\t' '{print \\$1\"\\t\"\\$5\"\\t\"\\$5}' locationstelomerelast.txt | sort -r | tr ' ' '\\t' > locationstelomerelast2.txt\n    grep -v 'seqID' locationstelomerelast2.txt > locationstelomerelast2.bed\n    \"\"\"\n}\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a9d243cb-8231-4f71-bd43-e48b5fa270d3",
      "cell_type": "code",
      "source": "##High Filter: Keeps only those reads that meet the strict filtering criteria (e.g., mapping position relative to telomere sites and cut sites).\n##Low Filter: Keeps reads with a more lenient set of criteria (e.g., the end position of the read is within 80 bp beyond the last telomere motif).\n##No Filter: Keeps all reads without any filtering (i.e., all reads from the input BAM are included)\n\nprocess filtering {\n    tag { meta.alias }\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 2.GB\n    input:\n        tuple val(meta), path(\"align.bam\"), path(\"align.bam.bai\")\n        tuple val(meta2), path(\"cutsites.txt\")\n        tuple val(meta2), path(\"telomeresites.txt\")\n    output:\n        tuple val(meta), path(\"highfiltered.bam\"), path(\"highfiltered.bam.bai\"), emit: finalbam\n        tuple val(meta), path(\"lowfiltered.bam\"), path(\"lowfiltered.bam.bai\"), emit: lowfinalbam\n        tuple val(meta), path(\"nofiltered.bam\"), path(\"nofiltered.bam.bai\"), emit: nofinalbam\n        tuple val(meta),\n            path(\"highfiltered.bam\"),\n            path(\"highfiltered.bam.bai\"),\n            path(\"lowfiltered.bam\"),\n            path(\"lowfiltered.bam.bai\"),\n            path(\"nofiltered.bam\"),\n            path(\"nofiltered.bam.bai\"),\n            emit: combined\n    script:\n    //Identify reads to be removed from bam based upon filtering criteria.\n    //['chr21_PATERNAL_P', 'chr21_MATERNAL_P'] have very long cut sitesm so 45000 length cut site limit is set so then strict filter does not just remove these type of contigs reads.\n    //no filter is no reads removed \n    //low filter is keep only reads in which there end mapping position is 80 bp beyond last telomere motif\n    //high filter is keep only reads in which there start mapping position is before last telomere motif identification and end is within 25 bp of cutsite\n\n##High filter entonces quitaría los telómeros que en otras líneas celulares valgan porque el cut site de la enzima este en otro punto, pero como no tenemos su genoma de referencia lo perdemos. Coger siempre no filter o low filter. \n\n    \"\"\"\n    seqkit bam align.bam 2>seqkitbamout.txt\n    filter_bam_reads_output_id.py --strict seqkitbamout.txt cutsites.txt telomeresites.txt id.txt\n\n    awk '!seen[\\$0]++' id.txt > id2.txt \n    samtools view -N id2.txt -o temp.bam align.bam \n    samtools index temp.bam\n    samtools view -h -o highfiltered.bam temp.bam\n    samtools index highfiltered.bam\n\n    filter_bam_reads_output_id.py --lenient seqkitbamout.txt cutsites.txt telomeresites.txt lowid.txt \n\n    awk '!seen[\\$0]++' lowid.txt > lowid2.txt \n    samtools view -N lowid2.txt -o lowtemp.bam align.bam\n    samtools index lowtemp.bam\n    samtools view -h -o lowfiltered.bam lowtemp.bam\n    samtools index lowfiltered.bam\n\n    filter_bam_reads_output_id.py --none seqkitbamout.txt cutsites.txt telomeresites.txt noneid3.txt\n\n    awk '!seen[\\$0]++' noneid3.txt > noneid4.txt \n    samtools view -N noneid4.txt -o nonetemp4.bam align.bam\n    samtools index nonetemp4.bam\n    samtools view -h -o nofiltered.bam nonetemp4.bam\n    samtools index nofiltered.bam\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a8bfd46e-cb0c-4ebf-a9eb-b56ee71edb06",
      "cell_type": "code",
      "source": "process raw_telomere_analysis {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 5.GB\n    input:\n        tuple val(meta), path(\"telomere.fastq\")\n    output:\n        tuple val(meta), path(\"Sample_raw_Coverage.csv\"), emit: covraw\n        tuple val(meta), path(\"Sample_raw_Boxplot_of_Telomere_length.pdf\"), emit: pdfraw\n        tuple val(meta), path(\"Sample_raw_Per_Read_telomere_length.csv\"), emit: plotraw\n    script:\n    //search for locations of telomere sequences (x5 repeats) in individual reads.\n    //Reverse locations file to select last occurance of each telomere match, thereby selecting end position of telomere\n    //subtract the first telomere location from last so remove adapter length from telomere length.\n    \"\"\"\n    cat telomere.fastq | seqkit locate --only-positive-strand -m 1 -p TAACCCTAACCCTAACCCTAACCCTAACCC,AACCCTAACCCTAACCCTAACCCTAACCCT,ACCCTAACCCTAACCCTAACCCTAACCCTA,CCCTAACCCTAACCCTAACCCTAACCCTAA,CCTAACCCTAACCCTAACCCTAACCCTAAC,CTAACCCTAACCCTAACCCTAACCCTAACC > locationstelomere.txt\n    tac locationstelomere.txt | awk '!a[\\$1]++' > locationstelomerelast.txt\n    awk -F'\\t' '{print \\$1\" \"\\$6}' locationstelomerelast.txt | sort -r | tr ' ' '\\t' > end\n    awk -F'\\t' '!a[\\$1]++' locationstelomere.txt | awk -F'\\t' '{print \\$1\" \"\\$5}' | sort -r | tr ' ' '\\t' > start\n    awk 'BEGIN {OFS=\"\\t\"} FNR==NR {if (NR>1) {a[\\$1]=\\$2}; next} FNR==1 {print} FNR>1 {\\$2=\\$2-a[\\$1]; print}' start end > Sample\n    telomerewindowV1.py telomere.fastq telomere_read_length.txt\n    telomere_length_coverage_raw2.py Sample telomere_read_length.txt\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dfca8caf-b3fc-4c4b-b014-f6c3518a3509",
      "cell_type": "code",
      "source": "process results {\n    tag { meta.alias }\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 2.GB\n    input:\n        tuple val(meta),\n            path(\"highfiltered.bam\"),\n            path(\"highfiltered.bam.bai\"),\n            path(\"lowfiltered.bam\"),\n            path(\"lowfiltered.bam.bai\"),\n            path(\"nofiltered.bam\"),\n            path(\"nofiltered.bam.bai\"),\n            path(\"raw_coverage.csv\"),\n            path(\"coverage.txt\")\n        tuple val(meta2), path(\"mapping_ref.fasta\")\n    output:\n        tuple val(meta),\n            path(\"highfiltered_chr_arm_Coverage.csv\"),\n            path(\"highfiltered_Per_Read_telomere_length.csv\"),\n            path(\"lowfiltered_chr_arm_Coverage.csv\"),\n            path(\"lowfiltered.csv\"),\n            path(\"lowfiltered_Per_Read_telomere_length.csv\"),\n            path(\"nofiltered_chr_arm_Coverage.csv\"),\n            path(\"nofiltered_Per_Read_telomere_length.csv\"),\n            path(\"output.csv\"), \n            emit: for_report\n        tuple val(meta),\n            path(\"highfiltered_Boxplot_of_Telomere_length.pdf\"),\n            path(\"highfiltered_chr_arm_Boxplot_of_Telomere_length.pdf\"),\n            path(\"nofiltered_chr_arm_Boxplot_of_Telomere_length.pdf\"),\n            path(\"lowfiltered_chr_arm_Boxplot_of_Telomere_length.pdf\"),\n            path(\"nofiltered_Boxplot_of_Telomere_length.pdf\"),\n            path(\"lowfiltered_Boxplot_of_Telomere_length.pdf\"),\n            emit: pdf\n        tuple val(meta),\n            path(\"nofiltered_Per_Read_telomere_length.csv\"),\n            path(\"highfiltered_chr_arm_Coverage.csv\"),\n            path(\"lowfiltered_Per_Read_telomere_length.csv\"),\n            path(\"highfiltered_Per_Read_telomere_length.csv\"),\n            path(\"nofiltered_chr_arm_Coverage.csv\"),\n            path(\"lowfiltered_chr_arm_Coverage.csv\"),\n            emit: alldata\n    script:\n    //search for locations of telomere sequences (x5 repeats) in individual reads.\n    //Reverse locations file to select last occurance of each telomere match, thereby selecting end position of telomere\n    //remove the first location of telomere from the last to remove any adapter length contributing.\n    \"\"\"\n    samtools index highfiltered.bam\n    samtools fastq highfiltered.bam | seqkit locate --only-positive-strand -m 1 -p TAACCCTAACCCTAACCCTAACCCTAACCC,AACCCTAACCCTAACCCTAACCCTAACCCT,ACCCTAACCCTAACCCTAACCCTAACCCTA,CCCTAACCCTAACCCTAACCCTAACCCTAA,CCTAACCCTAACCCTAACCCTAACCCTAAC,CTAACCCTAACCCTAACCCTAACCCTAACC > locationstelomerestrict.txt\n    tac locationstelomerestrict.txt | awk '!a[\\$1]++' > locationstelomerelaststrict.txt\n    awk -F'\\t' '{print \\$1\" \"\\$6}' locationstelomerelaststrict.txt | sort -r | tr ' ' '\\t' > strict_end\n    awk -F'\\t' '!a[\\$1]++' locationstelomerestrict.txt | awk -F'\\t' '{print \\$1\" \"\\$5}' | sort -r | tr ' ' '\\t' > strict_start\n    awk 'BEGIN {OFS=\"\\t\"} FNR==NR {if (NR>1) {a[\\$1]=\\$2}; next} FNR==1 {print} FNR>1 {\\$2=\\$2-a[\\$1]; print}' strict_start strict_end > strict_tel_length\n    samtools faidx mapping_ref.fasta\n    seqkit bam highfiltered.bam 2>highfiltered\n    samtools fastq highfiltered.bam > highfiltered.fastq\n    telomerewindowV1.py highfiltered.fastq high_telomere_read_length.txt\n    telomere_length_coverage3.py highfiltered strict_tel_length mapping_ref.fasta.fai coverage.txt high_telomere_read_length.txt\n\n    samtools index lowfiltered.bam\n    samtools fastq lowfiltered.bam | seqkit locate --only-positive-strand -m 1 -p TAACCCTAACCCTAACCCTAACCCTAACCC,AACCCTAACCCTAACCCTAACCCTAACCCT,ACCCTAACCCTAACCCTAACCCTAACCCTA,CCCTAACCCTAACCCTAACCCTAACCCTAA,CCTAACCCTAACCCTAACCCTAACCCTAAC,CTAACCCTAACCCTAACCCTAACCCTAACC > locationstelomerelenient.txt\n    tac locationstelomerelenient.txt | awk '!a[\\$1]++' > locationstelomerelastlenient.txt\n    awk -F'\\t' '{print \\$1\" \"\\$6}' locationstelomerelastlenient.txt | sort -r | tr ' ' '\\t' > lenient_end\n    awk -F'\\t' '!a[\\$1]++' locationstelomerelenient.txt | awk -F'\\t' '{print \\$1\" \"\\$5}' | sort -r | tr ' ' '\\t' > lenient_start\n    awk 'BEGIN {OFS=\"\\t\"} FNR==NR {if (NR>1) {a[\\$1]=\\$2}; next} FNR==1 {print} FNR>1 {\\$2=\\$2-a[\\$1]; print}' lenient_start lenient_end > lenient_tel_length\n    seqkit bam lowfiltered.bam 2>lowfiltered\n    samtools fastq lowfiltered.bam > lowfiltered.fastq\n    telomerewindowV1.py lowfiltered.fastq low_telomere_read_length.txt \n\n    telomere_length_coverage3.py lowfiltered lenient_tel_length mapping_ref.fasta.fai coverage.txt low_telomere_read_length.txt\n\n    samtools index nofiltered.bam\n    samtools fastq nofiltered.bam | seqkit locate --only-positive-strand -m 1 -p TAACCCTAACCCTAACCCTAACCCTAACCC,AACCCTAACCCTAACCCTAACCCTAACCCT,ACCCTAACCCTAACCCTAACCCTAACCCTA,CCCTAACCCTAACCCTAACCCTAACCCTAA,CCTAACCCTAACCCTAACCCTAACCCTAAC,CTAACCCTAACCCTAACCCTAACCCTAACC > locationstelomereraw.txt\n    tac locationstelomereraw.txt | awk '!a[\\$1]++' > locationstelomerelastraw.txt\n    awk -F'\\t' '{print \\$1\" \"\\$6}' locationstelomerelastraw.txt | sort -r | tr ' ' '\\t' > raw_end\n    awk -F'\\t' '!a[\\$1]++' locationstelomereraw.txt | awk -F'\\t' '{print \\$1\" \"\\$5}' | sort -r | tr ' ' '\\t' > raw_start\n    awk 'BEGIN {OFS=\"\\t\"} FNR==NR {if (NR>1) {a[\\$1]=\\$2}; next} FNR==1 {print} FNR>1 {\\$2=\\$2-a[\\$1]; print}' raw_start raw_end > raw_tel_length\n    seqkit bam nofiltered.bam 2>nofiltered\n    samtools fastq nofiltered.bam > nofiltered.fastq\n    telomerewindowV1.py nofiltered.fastq no_telomere_read_length.txt\n\n    telomere_length_coverage3.py nofiltered raw_tel_length mapping_ref.fasta.fai coverage.txt no_telomere_read_length.txt\n\n    combine_files.py \n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9fcecd92-27ae-40de-bff6-a547e505a30a",
      "cell_type": "code",
      "source": "process makeReport {\n    label \"wf_teloseq\"\n    cpus   = 1\n    memory 2.GB\n    input:\n        val metadata\n        path \"versions/*\"\n        path \"params.json\"\n        path \"data/*\"\n        val mappingreport\n    output:\n        path \"wf-teloseq-*.html\"\n    script:\n        String extra_arg = \"\"\n        if (mappingreport) {\n            extra_arg = \"--mappingreport\"\n        }\n        String report_name = \"wf-teloseq-report.html\"\n        String metadata = new JsonBuilder(metadata).toPrettyString()\n    \"\"\"\n    echo '${metadata}' > metadata.json\n\n    workflow-glue report $report_name \\\n        --metadata metadata.json \\\n        --versions versions \\\n        --params params.json \\\n        --data data \\\n        $extra_arg\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1bcdfa0b-ef5b-4e53-a7a6-b94d55a8b2c9",
      "cell_type": "code",
      "source": "// See https://github.com/nextflow-io/nextflow/issues/1636. This is the only way to\n// publish files from a workflow whilst decoupling the publish from the process steps.\n// The process takes a tuple containing the filename and the name of a sub-directory to\n// put the file into. If the latter is `null`, puts it into the top-level directory.\nprocess output {\n    // publish inputs to output directory\n    label \"wf_teloseq\"\n    cpus 1\n    memory 8.GB\n    publishDir (\n        params.out_dir,\n        mode: \"copy\",\n        saveAs: { dirname ? \"$dirname/$fname\" : fname }\n    )\n    input:\n        tuple path(fname), val(dirname)\n    output:\n        path fname\n    \"\"\"\n    \"\"\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "367fd0a0-5cda-4951-beea-c73d2617ae5e",
      "cell_type": "code",
      "source": "process collectFilesInDir {\n    label \"wf_teloseq\"\n    cpus 1\n    memory 8.GB\n    input:\n        tuple val(meta), path(\"staging_dir/*\"), val(dirname)\n    output:\n        tuple val(meta), path(dirname)\n    script:\n    \"\"\"\n    mv staging_dir $dirname\n    \"\"\"\n}\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2710a7f7-d3d3-4f89-80ac-3021aecd397a",
      "cell_type": "code",
      "source": "// workflow module\nworkflow pipeline {\n    take:\n        samples\n    main:\n        software_versions = getVersions()\n        workflow_params = getParams()\n        metadata = samples.map { meta, reads, stats -> meta }.toList()\n\n        // Read file and create metadata tuple\n        Path ref = file(params.reference ?: \"$projectDir/test_data/HG002qpMP_reference.fasta.gz\", checkIfExists: true)\n\n###Alternativa para usar la otra referencia:\n\n\n\n        //enzyme cut site to use\n        enzyme_cut_site = params.enzyme_cut\n\n        // Pass the channel to the check_reference process\n        //check_reference(ref_ch)\n        check_reference(ref,enzyme_cut_site)\n        \n        //remove duplicate reads - should not be common but just in case\n        dedup = rmdup(samples.map{ meta, reads, stats -> [ meta,reads ] })\n\n        //filter for telomere containing reads for 10 repeats within 60-500 bp of read\n        telomeres = filter_telomeres(dedup)\n\n        if (params.doublestranded) {\n            //if both strands\n            reversedreads=reversecomplement(dedup)\n            telomeres2 = filter_telomeres2(reversedreads)\n            telomeres3=combinefastq(telomeres,telomeres2)\n            telomeres4=rmdup2(telomeres3)\n            //filter for no telomere sequence 5 repeats for the last 60bp. Limited to 60 as cutsite for 2 chr arms is only 80bp from telomere. \n            //This is to filter out telomere only reads with no subtelomere to use for mapping and avoid telomere fragments.\n            nontelomeres = filter_nontelomeres(telomeres4)\n            remove_short_telomeres = remove_short2(telomeres4)\n        } else {\n            nontelomeres = filter_nontelomeres(telomeres)\n            remove_short_telomeres = remove_short2(telomeres)\n        }\n\n        //remove short reads\n        remove_short_nontelomeres = remove_short1(nontelomeres)\n\n        //get subtelomere length information for plot\n        sub1 = subtelomere(remove_short_nontelomeres)\n\n        //This identifies reads with basecalling error to remove from the pipeline\n        filtered = filter_motifs(remove_short_telomeres) \n\n## Esto es lo que no me gusta, que elimine algunos reads que considere tienen errores en basecalling pero que podrían ser simples mutaciones.\n\n        //filtered telomere read fastq and filtered telomere-subtelomere fastq\n        t1 = filter_motifs_reads1(remove_short_telomeres.join(filtered))\n        t2 = filter_motifs_reads2(remove_short_nontelomeres.join(filtered))\n\n        trim_adapters(t2)\n\n        //telomere lengths plot for raw filtered error reads\n        raw_telomere_analysis(trim_adapters.out.fastqtrimmed)\n\n        //get min coverage\n        read_count = trim_adapters.out.fastqtrimmed.countLines()\n\n        //hard coded minimum read count for clustering, I did calculate on 20% of chr arm but when cov low then minimum read number would lead to inflated contigs. Default is 8 reads\n        //I worry if go too low then snps/indels lead to novel contigs but are the same chr arm.  3000 telomere reads should be ~32 per chr arm so 8 is 25% but this is number\n        //of clustered reads so not directly relatable and its how well I separated out the reads which won't be 100, it might be 50%.\n        cov = params.cov_4cluster\n\n        //coverage of final chr arm needs to be at least 20% average coverage otherwise likely duplicate contig.\n        cov_filter=coverage_calc(trim_adapters.out.fastqtrimmed)\n        \n        //get stats on raw reads, telomere containing reads, exclude telomere only reads. Put output into variable for report.\n        read_stats1 = fastq_stats(dedup)\n        read_stats2 = fastq_stats2(t1)\n        read_stats3 = fastq_stats3(trim_adapters.out.fastqtrimmed)\n\n\n        //output results to channel for copying\n        ch_to_publish = Channel.empty()\n        | mix(\n            software_versions | map { [it, null] },\n            workflow_params | map { [it, null] },\n        )\n\n        //add to output channel telomere reads\n        ch_to_publish = ch_to_publish\n        | mix(\n            trim_adapters.out.fastqtrimmed \n            | map { meta, reads -> [reads, \"${meta.alias}/reads\"] }\n            | transpose\n        )\n\n        //add to output channel raw telomere results csv\n        ch_to_publish = ch_to_publish \n        | mix(\n            raw_telomere_analysis.out.plotraw \n            | map { meta, csv -> [csv, \"${meta.alias}/results\"] }\n            | transpose\n        )\n\n        //add to output channel raw telomere pdf plot\n        ch_to_publish = ch_to_publish \n        | mix(\n            raw_telomere_analysis.out.pdfraw \n            | map { meta, pdf -> [pdf, \"${meta.alias}/plots\"] }\n            | transpose\n        )\n\n        // NON MAPPING ROUTE REPORT  \n        if (params.skipmapping) {\n            // get all the per sample results together\n            ch_per_sample_results = samples\n            | map { meta, reads, stats_dir -> [meta, stats_dir] }\n            | join(read_stats1)\n            | join(read_stats2)\n            | join(read_stats3)\n            | join(raw_telomere_analysis.out.plotraw)\n            | join(raw_telomere_analysis.out.covraw)\n            | join(sub1)\n\n            // collect results into a directory for the sample directory to avoid collisions\n            ch_results_for_report = ch_per_sample_results\n            | map {\n                meta = it[0]\n                rest = it[1..-1]\n                [meta, rest, meta.alias]\n            }\n            | collectFilesInDir\n            | map { meta, dirname -> dirname }\n\n            //make report html with all information\n            mappingreport=false\n            report = makeReport(\n                metadata,  \n                software_versions,\n                workflow_params,\n                ch_results_for_report | collect,\n                mappingreport\n            )\n        } else {\n            //MAPPING ARM PIPELINE\n\n            //denovo reference route\n            if (params.denovo) {\n                //remove subtelomere for raw and reference and map, output read subsets based upon mapping.\n                clusterset = mapAndSeparateR1(trim_adapters.out.fastqtrimmed, check_reference.out.ref1, cov)\n                //separate out each subset read file to a tuple in the new channel\n                split_clusters = clusterset\n                .flatMap { tuple ->\n                // //tuple[0] is the list of four values and tuple[1] is the list of paths\n                metadata = tuple[0]\n                paths = tuple[1]\n                //Create a new tuple for each path with the same key\n                paths.collect { path -> [metadata, path] }\n                }\n                //combine to a channel so the fastq file with each text file so runs each rather than just once with one fastq file\n                clusterchannel = split_clusters.combine(trim_adapters.out.fastqtrimmed, by: 0)\n                //group tuple should collect all results with key. This is the vsearch clustering step for each set of reads identified as chr arm\n                clusterout = clusterAndExtractR1(clusterchannel, cov).groupTuple()    \n                //collect by groupTuple the files from each meta key to collate the contigs into one reference\n                combineRefR1(trim_adapters.out.fastqtrimmed.join(clusterout), cov)\n\n                /////////////////////////////////////////////////////////////////\n                //phase 1 of clustering and assembling denovo reference done\n                ////////////////////////////////////////////////////////////////\n\n\n                //second round of phasing reads to cluster\n                phaseagain = mapAndSeparateR2(combineRefR1.out.bam, cov)\n                //separate out each file to a tuple in the new channel\n                split_clusters2 = phaseagain\n                .flatMap { tuple ->\n                // //tuple[0] is the list of four values and tuple[1] is the list of paths\n                metadata = tuple[0]\n                paths = tuple[1]\n                //Create a new tuple for each path with the same key\n                paths.collect { path -> [metadata, path] }\n                }\n                //combine to a channel so the fastq file with each text file so runs each rather than just once with one fastq file\n                clusterchannel2 = split_clusters2.combine(trim_adapters.out.fastqtrimmed, by: 0)\n                //group tuple should collect all results with key. This is the vsearch clustering step for each set of reads identified as chr arm\n                clusterout2 = clusterAndExtractR2(clusterchannel2, cov).groupTuple()  \n                //collect by groupTuple the files from each meta key to collate the contigs into one reference\n                combineRefR2(\n                    trim_adapters.out.fastqtrimmed\n                    | join(clusterout2)\n                    | join(combineRefR1.out.ref1)\n                    | join(cov_filter),\n                    check_reference.out.ref1,\n                    enzyme_cut_site,\n                )\n\n                /////////////////////////////////////////////////////////////////\n                //phase 2 of clustering and assembling denovo reference done\n                ////////////////////////////////////////////////////////////////\n\n\n                //second round of phasing reads to cluster\n                phaseagain2 = mapAndSeparateR3(combineRefR2.out.bam, cov)\n                //separate out each file to a tuple in the new channel\n                split_clusters3 = phaseagain2\n                .flatMap { tuple ->\n                // //tuple[0] is the list of four values and tuple[1] is the list of paths\n                metadata = tuple[0]\n                paths = tuple[1]\n                //Create a new tuple for each path with the same key\n                paths.collect { path -> [metadata, path] }\n                }\n\n\n                //combine to a channel so the fastq file with each text file so runs each rather than just once with one fastq file\n                clusterchannel3 = split_clusters3.combine(trim_adapters.out.fastqtrimmed, by: 0)\n\n                //group tuple should collect all results with key. This is the vsearch clustering step for each set of reads identified as chr arm\n                clusterout3 = clusterAndExtractR3(clusterchannel3, cov).groupTuple()  \n\n                //collect by groupTuple the files from each meta key to collate the contigs into one reference\n                combineRefR3(\n                    trim_adapters.out.fastqtrimmed\n                    | join(clusterout3)\n                    | join(combineRefR2.out.ref1)\n                    | join(cov_filter),\n                    check_reference.out.ref1,\n                    enzyme_cut_site,\n                )\n\n                /////////////////////////////////////////////////////////////////\n                //phase 3 of clustering and assembling denovo reference done\n                ////////////////////////////////////////////////////////////////\n\n\n                //ref1_paths = combineRefR2.out.ref1.map { path -> [meta, path] }     \n                //ref1_paths = combineRefR3.out.ref1.map { tuple -> tuple[1] }\n                //map filtered telomere reads to genome and filter using mapq (default=10)\n                mappingbam(trim_adapters.out.fastqtrimmed, combineRefR3.out.ref1)\n                //last telomere repeat location on the reference from the enzyme for each chr\n                telomere_sites(combineRefR3.out.ref1)\n                //filter bam with high, low and no stringency but including mapping quality filter applied in previous step\n                filtering(mappingbam.output.alignments, combineRefR3.out.cutbed, telomere_sites.out.telomerebed)\n                //get final telomere stats\n                results(\n                    filtering.out.combined\n                    | join(raw_telomere_analysis.out.covraw)\n                    | join(cov_filter),\n                    combineRefR3.out.ref1,\n                )\n\n                //add to output channel\n                ch_to_publish = ch_to_publish \n                | mix(\n                    combineRefR3.out.ref1 \n                    | map { meta, ref1  -> [[ref1], \"$meta.alias/reference\"] }\n                    | transpose\n                )\n            } else {\n                //using reference provided rather than de novo route.\n                if (params.curation) {\n                    //merge and correct de novo reference and new contigs\n                    contigstoadd=file(params.curatedContigs, checkIfExists: true)\n                    denovoref1=file(params.denovoRef, checkIfExists: true)\n                    // TODO: do we need to make sure that `params.curatedContigs` and\n                    // `params.denovoRef` are present when `params.curation`? if so, we\n                    // should do this in the schema\n                    manualCuration(\n                        trim_adapters.out.fastqtrimmed\n                        | join(cov_filter),\n                        contigstoadd,\n                        denovoref1,\n                        check_reference.out.ref1,\n                        enzyme_cut_site,\n                    )\n                    //last telomere repeat location on the reference from the enzyme for each chr\n                    //ref1_paths = manualCuration.out.ref1.map { tuple -> tuple[1] }\n                    telomere_sites(manualCuration.out.ref1)\n                    //first cutsite location on the reference from the enzyme for each chr\n                    cut_sites(manualCuration.out.ref1,enzyme_cut_site)\n                    //map filtered telomere reads to genome and filter using mapq (default=10)\n                    mappingbam(trim_adapters.out.fastqtrimmed, manualCuration.out.ref1)\n                    //filter bam with high, low and no stringency but including mapping quality filter applied in previous step\n                    filtering(mappingbam.output.alignments, cut_sites.out.cutbed, telomere_sites.out.telomerebed)\n                    //get final telomere stats\n                    results(\n                        filtering.out.combined\n                        | join(raw_telomere_analysis.out.covraw)\n                        | join(cov_filter),\n                        manualCuration.out.ref1\n                    )\n\n                    //add to output channel\n                    ch_to_publish = ch_to_publish \n                        | mix(\n                        manualCuration.out.ref1 \n                        | map { meta, ref1  -> [[ref1], \"$meta.alias/reference\"] }\n                        | transpose\n                    )\n                } else {\n                    //last telomere repeat location on the reference from the enzyme for each chr\n                    telomere_sites(check_reference.out.ref1)\n                    //first cutsite location on the reference from the enzyme for each chr\n                    cut_sites(check_reference.out.ref1,enzyme_cut_site)\n                    //map filtered telomere reads to genome and filter using mapq (default=10)\n                    mappingbam(trim_adapters.out.fastqtrimmed, check_reference.out.ref1)\n                    //filter bam with high, low and no stringency but including mapping quality filter applied in previous step\n                    filtering(mappingbam.output.alignments, cut_sites.out.cutbed, telomere_sites.out.telomerebed)\n                    //get final telomere stats\n                    results(\n                        filtering.out.combined\n                        | join(raw_telomere_analysis.out.covraw)\n                        | join(cov_filter),\n                        check_reference.out.ref1,\n                    )\n                }\n            }\n\n            // get all the per sample results together\n            ch_per_sample_results = samples\n            | map { meta, reads, stats_dir -> [meta, stats_dir] }\n            | join(read_stats1)\n            | join(read_stats2)\n            | join(read_stats3)\n            | join(raw_telomere_analysis.out.plotraw) \n            | join(raw_telomere_analysis.out.covraw)\n            | join(sub1)\n            | join(results.out.for_report)\n\n            // collect results into a directory for the sample directory to avoid collisions\n            ch_results_for_report = ch_per_sample_results\n            | map {\n                meta = it[0]\n                rest = it[1..-1]\n                [meta, rest, meta.alias]\n            }\n            | collectFilesInDir\n            | map { meta, dirname -> dirname }\n\n            //make report html file with all information\n            mappingreport=true\n            report = makeReport(\n                    metadata,\n                    software_versions,\n                    workflow_params,\n                    ch_results_for_report | collect,\n                    mappingreport\n                )\n\n            //add to output channel, bam alignment files\n            ch_to_publish = ch_to_publish\n                | mix(\n                    mappingbam.out.alignments\n                    | map { meta, bam, bai -> [[bam, bai], \"$meta.alias/alignments\"] }\n                    | transpose\n                )\n\n            //add to output channel, mapped csv files\n            ch_to_publish = ch_to_publish \n                | mix(\n                results.out.alldata \n                | map { meta, csv1,csv2,csv3,csv4,csv5 ,csv6 -> [[csv1, csv2, csv3, csv4, csv5, csv6], \"${meta.alias}/results\"] }\n                | transpose\n            )\n\n            //add to output channel, mapped pdf plot files\n            ch_to_publish = ch_to_publish \n                | mix(\n                results.out.pdf \n                | map { meta, pdf1,pdf2,pdf3,pdf4,pdf5 ,pdf6 -> [[pdf1, pdf2, pdf3, pdf4, pdf5, pdf6], \"${meta.alias}/plots\"] }\n                | transpose\n            )\n            //add to output channel, reference used for mapping final results\n            ch_to_publish = ch_to_publish \n                | mix(\n                mappingbam.out.mappingref \n                | map { meta, mappingref  -> [[mappingref], \"$meta.alias/alignments\"] }\n                | transpose\n            )\n            \n            //add to output channel, filtered strict bam files\n            ch_to_publish = ch_to_publish \n                | mix(\n                filtering.out.finalbam \n                | map { meta, bam, bai  -> [[bam, bai], \"$meta.alias/alignments\"] }\n                | transpose\n            )\n\n            //add to output channel, filtered lenient bam files\n            ch_to_publish = ch_to_publish \n                | mix(\n                filtering.out.lowfinalbam \n                | map { meta, bam, bai  -> [[bam, bai], \"$meta.alias/alignments\"] }\n                | transpose\n            )\n        }\n\n    //this emits the report, files to output directory and telemetry information\n    emit:\n        report\n        combined_results_to_publish = ch_to_publish\n        workflow_params\n        telemetry = workflow_params\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0732d31-66bd-4982-993b-7940a3c23fc0",
      "cell_type": "code",
      "source": "// entrypoint workflow\nWorkflowMain.initialise(workflow, params, log)\nworkflow {\n\n    Pinguscript.ping_start(nextflow, workflow, params)\n\n    // demo mutateParam\n    if (params.containsKey(\"mutate_fastq\")) {\n        CWUtil.mutateParam(params, \"fastq\", params.mutate_fastq)\n    }\n\n    Map ingress_args = [\n        \"sample\":params.sample,\n        \"sample_sheet\":params.sample_sheet,\n        \"analyse_unclassified\":params.analyse_unclassified,\n        \"stats\": true,   // TODO: we might wanna use these instead of the seqkit stats\n        \"fastcat_extra_args\": \"\",   // TODO: we could use this to filter based on read length + quality\n    ]\n    if (params.fastq) {\n        samples = fastq_ingress(ingress_args + [\n            \"input\":params.fastq\n        ])\n    } else {\n        // if we didn't get a `--fastq`, there must have been a `--bam` (as is codified\n        // by the schema)\n        samples = xam_ingress(ingress_args + [\n            \"input\":params.bam,\n            \"return_fastq\": true,\n            \"keep_unaligned\": true,\n        ])\n    }\n\n    pipeline(samples)\n\n    // publish results\n    pipeline.out.combined_results_to_publish\n    | toList\n    | flatMap | concat (\n        pipeline.out.report.concat(pipeline.out.workflow_params)\n        | map { [it, null] }\n    )\n    | output\n\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eca27f54-e3e6-4fb4-9143-a60935caa70a",
      "cell_type": "code",
      "source": "workflow.onComplete {\n    Pinguscript.ping_complete(nextflow, workflow, params)\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "37fb3fbe-c0ce-46d3-bf3d-2d1bb59869ea",
      "cell_type": "code",
      "source": "workflow.onError {\n    Pinguscript.ping_error(nextflow, workflow, params)\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}